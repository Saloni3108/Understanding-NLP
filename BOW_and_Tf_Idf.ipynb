{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BOW and Tf-Idf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0PEBTPBBXDI"
      },
      "source": [
        "# Encode Text data for Machine Learning\n",
        "* The text must be parsed to remove words through tokenization\n",
        "* Then the words needs to be transformed to integer or floating number before fitting in the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8q6wYUoBuBJ"
      },
      "source": [
        "#Bag of Words\n",
        "* This model is focused around the occurence or count of words or the degree to which the words is present within the document and across the documents.\n",
        "* Three methoods - Count Vectorizer, Tf-Idf Vectorizer and Hashing Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B4_u3yKBtaI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = ['The quick brown fox jumped over the lazy dog.']\n",
        "cv = CountVectorizer()\n",
        "vector = cv.fit_transform(text)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqmadGZwCgTx",
        "outputId": "64984361-1133-44a3-d8d5-64659c7063cd"
      },
      "source": [
        "print(cv.vocabulary_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLyA3rlwCqXr",
        "outputId": "ed7ddd1f-7f18-48fb-f3ae-081e2f7c47cb"
      },
      "source": [
        "print('Shape of vector::',vector.shape)\n",
        "print('Array::',vector.toarray())\n",
        "print(type(vector))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of vector:: (1, 8)\n",
            "Array:: [[1 1 1 1 1 1 1 2]]\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea0rxVEaDAu3"
      },
      "source": [
        "# Tf-IDF Vectorization\n",
        "One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
        "\n",
        "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "Term Frequency: This summarizes how often a given word appears within a document.\n",
        "\n",
        "Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ozfdtcC7kT",
        "outputId": "a004d49b-96f0-4932-8f50-d65307fc7aba"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "#text = [\"the quick brown fox jumped over the lazy dog.\"]\n",
        "\n",
        "\n",
        "\n",
        "text = [\"the quick brown fox jumped over the lazy dog.\",\n",
        "\t\t\"the dog.\",\n",
        "\t\t\"the fox\"]\n",
        "\n",
        "\n",
        "\n",
        "#text = [\"the quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnFgJ4jDQrL"
      },
      "source": [
        "* Above a vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
        "\n",
        "* The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: “the” at index 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXW0uxtFDI3F",
        "outputId": "da09c193-a2ca-4128-cce1-099e3686e113"
      },
      "source": [
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGm_65lrDW3W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}